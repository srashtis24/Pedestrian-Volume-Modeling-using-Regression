f(1, 2)
## We can also run this function while naming the input arguments:
f(x = 1, y = 2) # same output
## We can try every approach of inputting arguments:
f(x = 1, y = 2) # both named
f(x = 1, 2) # first named
f(1, y = 2) # second named
f(1, 2) # neither named
## But if we omit the second argument, what happens?
f(x = 1)
## So, let's define a new function which has a default argument:
f2 <- function(x, y = 2){
(x + y) / y
}
## This time, we only need to pass in an input for the first argument, x:
f2(x = 1)
f2(1) # We don't need to name x to get the same output
## What does this for-loop do?
for (x in 1:5) {
print(x)
}
## How about this loop?
for (iterator in c(2, 4, 6, 7)) {
cube <- iterator^3
print(cube + 2)
}
## We don't have to just loop over integers - we can also loop over characters!
animal_names <- c("Cat", "Dog", "Pig", "Elephant", "Giraffe")
## We'll use a for-loop to calculate the length of each name, and store it in a new vector.
for (name in animal_names) {
print(nchar(name))
}
## We'll use a for-loop to calculate the length of each name, and store it in a new vector.
for (name in animal_names) {
print(nchar(name))
}
nchar?
?nchar()
## Notice how the structure of an if-statement is very similar to a for-loop. The main difference is that the parentheses contain a conditional, rather than an iteration. If the conditional evaluates to TRUE, the body of the if-statement runs. If not, it is skipped. Here's another example using a for-loop from before:
for (x in 1:5) {
if (x < 3) {
print(paste0("x = ", x, ", is less than 3."))
}
}
## Notice how the structure of an if-statement is very similar to a for-loop. The main difference is that the parentheses contain a conditional, rather than an iteration. If the conditional evaluates to TRUE, the body of the if-statement runs. If not, it is skipped. Here's another example using a for-loop from before:
for (x in 1:5) {
if (x < 3) {
print(paste0("x = ", x, ", is less than 3."))
}
}
## An else-statement is used to specify what should occur when the if-statement is not satisfied.
x <- 1
## Does it appear in your global environment? (It should not.)
ls()
## OK, maybe it's because we haven't yet called the function. What about when we run the function once - does `z` enter the global environment?
f(x = 4)
ls()
## We're going to run some experiments in R that mimic the rolling of a die. To do so, we need random number generation. The sample() function is one way to randomly draw numbers from a set of values:
?sample
# Let's sample one die roll:
sample(1:6, 1)
## We're going to run some experiments in R that mimic the rolling of a die. To do so, we need random number generation. The sample() function is one way to randomly draw numbers from a set of values:
?sample
## We can think of this as one "experiment". In Monte Carlo sampling, we want to run many experiments to see how things behave in the aggregate. Let's run 10 experiments, each of which has 100 die rolls.
n_experiments <- 10
n_rolls <- 100
## We're interested in the average die roll. So, we'll write a for loop to iterate over the experiments, each time calculating the average die roll:
for (i in 1:n_experiments) {
rolls <- sample(1:6, n_rolls, replace = TRUE)
print(mean(rolls))
}
## Now, we generate 25 samples from 365 days of the year:
birthdays <- sample(1:365, 25, replace = TRUE)
birthdays
## Let's check if any birthdays are duplicated:
duplicated(birthdays)
sum(duplicated(birthdays)) > 0
# Install necessary packages if not installed
install.packages("magrittr")
install.packages("keras")
install.packages("tensorflow")
install.packages("magick")  # For image processing
install.packages("tfdatasets")
# Install Keras and TensorFlow in R if not already installed
library(keras)
library(tensorflow)
library(magick)  # Using magick for image processing
library(tfdatasets)
library(magrittr)
# Install Keras and TensorFlow using reticulate (for TensorFlow backend in Keras)
install_keras()
validation_datagen <- image_data_generator(rescale = 1/255)
install_tensorflow()
# Install Keras and TensorFlow using reticulate (for TensorFlow backend in Keras)
install_keras()
# Install Keras and TensorFlow in R if not already installed
library(keras)
library(tensorflow)
library(magick)  # Using magick for image processing
library(tfdatasets)
library(magrittr)
# Install Keras and TensorFlow using reticulate (for TensorFlow backend in Keras)
install_keras()
install_tensorflow()
# Install necessary packages if not installed
install.packages("magrittr")
# Install Keras and TensorFlow in R if not already installed
library(keras)
library(tensorflow)
library(magick)  # Using magick for image processing
library(tfdatasets)
library(magrittr)
# Install Keras and TensorFlow using reticulate (for TensorFlow backend in Keras)
install_keras()
# Path to the AID dataset (update with your dataset directory)
dataset_dir <- "path_to_AID_dataset"  # Replace with the correct path
train_dir <- file.path(dataset_dir, "train")
validation_dir <- file.path(dataset_dir, "validation")
# Data Augmentation and Preprocessing using magick
train_datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 20,
width_shift_range = 0.2,
height_shift_range = 0.2,
shear_range = 0.2,
zoom_range = 0.2,
horizontal_flip = TRUE,
fill_mode = "nearest"
)
validation_datagen <- image_data_generator(rescale = 1/255)
# Load training and validation datasets
train_generator <- flow_images_from_directory(
train_dir,
train_datagen,
target_size = c(img_width, img_height),
batch_size = batch_size,
class_mode = "categorical"
)
# Define parameters
img_width <- 600
img_height <- 600
batch_size <- 32
num_classes <- 30  # Total number of classes in the AID dataset
# Path to the AID dataset (update with your dataset directory)
dataset_dir <- "path_to_AID_dataset"  # Replace with the correct path
train_dir <- file.path(dataset_dir, "train")
validation_dir <- file.path(dataset_dir, "validation")
# Data Augmentation and Preprocessing using magick
train_datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 20,
width_shift_range = 0.2,
height_shift_range = 0.2,
shear_range = 0.2,
zoom_range = 0.2,
horizontal_flip = TRUE,
fill_mode = "nearest"
)
validation_datagen <- image_data_generator(rescale = 1/255)
# Load training and validation datasets
train_generator <- flow_images_from_directory(
train_dir,
train_datagen,
target_size = c(img_width, img_height),
batch_size = batch_size,
class_mode = "categorical"
)
validation_generator <- flow_images_from_directory(
validation_dir,
validation_datagen,
target_size = c(img_width, img_height),
batch_size = batch_size,
class_mode = "categorical"
)
# Build the CNN model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(img_width, img_height, 3)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = num_classes, activation = "softmax")
# Compile the model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Train the model
history <- model %>% fit_generator(
train_generator,
steps_per_epoch = train_generator$n // train_generator$batch_size,
# Evaluate the model
score <- model %>% evaluate_generator(validation_generator, steps = validation_generator$n // validation_generator$batch_size)
cat("Validation Loss: ", score[1], "\n")
cat("Validation Accuracy: ", score[2], "\n")
setwd("C:/Users/Hp/Downloads/CE687_Assignment_1_ped_data_variables_code_c8ceed0a-9566-4969-b8cc-50a05fcfa32c/Assignment_1_ped_data_variables_code")#(i) Model Selection & Justification
getwd()
library(tidyverse)
install.packages("corrplot")
library(corrplot)
#Import data
pedvars <- read.csv("ped_exposure_data_sample.csv")
colnames(pedvars)
# split data into training and test data (80-20 split)
n<-nrow(pedvars)
set.seed(241030083) ##changing the seed will change the training/test data sets
index <- sample(n, round(n*0.8))
train <- pedvars[index,] #80% of data
test <- pedvars[-index,] #20% of data
##Comparing Model 1 (Linear) and Model 2 (Log-Linear).
#Model 1: Linear Regression (AnnualEst ~ PopT)
model_1 <- lm(AnnualEst~PopT,data=train)
summary(model_1)
###Model 2: Log-Linear Regression (logAnnualEst ~ PopT)
model_2 <- lm(logAnnualEst~PopT,data=train)
summary(model_2)
##To compare the models, we check:
####model 1 goodness-of-Fit
logLik(model_1)
AIC(model_1)
BIC(model_1)
#model 1: Prediction on test data
AnnualEst_pred_model_1<-predict(model_1,test)
#model 1: training RMSE calculation
model_1_rmse_train<-(sum(model_1$residuals^2)/nrow(train))^0.5
#model 1: test RMSE calculation
model_1_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_1)^2)/nrow(test))^0.5
####model 2 Goodness-of-Fit
logLik(model_2)
AIC(model_2)
BIC(model_2)
#model 2: Prediction on train and test data and converting back from log
AnnualEst_pred_model_2_train<-exp(predict(model_2,train))
AnnualEst_pred_model_2_test<-exp(predict(model_2,test))
#model 2: training RMSE of not log, but back-transformed estimates
model_2_rmse_train<-(sum((train$AnnualEst-AnnualEst_pred_model_2_train)^2)/nrow(train))^0.5
#model 2: test RMSE
model_2_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_2_test)^2)/nrow(test))^0.5
## Display results
data.frame(Metric = c("R²", "Adjusted R²", "AIC", "BIC", "RMSE (Train)", "RMSE (Test)"),
Model_1 = c(summary(model_1)$r.squared, summary(model_1)$adj.r.squared,
AIC(model_1), BIC(model_1), train_rmse1, test_rmse1),
Model_2 = c(summary(model_2)$r.squared, summary(model_2)$adj.r.squared,
AIC(model_2), BIC(model_2), train_rmse2, test_rmse2))
###corrplot
colnames(train)
vars<-train[,6:21]
corrmat<-cor(vars)
corrplot(corrmat) # default is circle
setwd("C:/Users/Hp/Downloads/CE687_Assignment_1_ped_data_variables_code_c8ceed0a-9566-4969-b8cc-50a05fcfa32c/Assignment_1_ped_data_variables_code")#(i) Model Selection & Justification
getwd()
library(tidyverse)
#install.packages("corrplot")
library(corrplot)
#Import data
pedvars <- read.csv("ped_exposure_data_sample.csv")
colnames(pedvars)
# split data into training and test data (80-20 split)
n<-nrow(pedvars)
set.seed(241030083) ##changing the seed will change the training/test data sets
index <- sample(n, round(n*0.8))
train <- pedvars[index,] #80% of data
test <- pedvars[-index,] #20% of data
##Comparing Model 1 (Linear) and Model 2 (Log-Linear).
#Model 1: Linear Regression (AnnualEst ~ PopT)
model_1 <- lm(AnnualEst~PopT,data=train)
summary(model_1)
###Model 2: Log-Linear Regression (logAnnualEst ~ PopT)
model_2 <- lm(logAnnualEst~PopT,data=train)
summary(model_2)
setwd("C:/Users/Hp/Downloads/CE687_Assignment_1_ped_data_variables_code_c8ceed0a-9566-4969-b8cc-50a05fcfa32c/Assignment_1_ped_data_variables_code")#(i) Model Selection & Justification
getwd()
library(tidyverse)
#install.packages("corrplot")
library(corrplot)
#Import data
pedvars <- read.csv("ped_exposure_data_sample.csv")
colnames(pedvars)
# split data into training and test data (80-20 split)
n<-nrow(pedvars)
set.seed(241030083) ##changing the seed will change the training/test data sets
index <- sample(n, round(n*0.8))
train <- pedvars[index,] #80% of data
test <- pedvars[-index,] #20% of data
##Comparing Model 1 (Linear) and Model 2 (Log-Linear).
#Model 1: Linear Regression (AnnualEst ~ PopT)
model_1 <- lm(AnnualEst~PopT,data=train)
summary(model_1)
###Model 2: Log-Linear Regression (logAnnualEst ~ PopT)
model_2 <- lm(logAnnualEst~PopT,data=train)
summary(model_2)
##To compare the models, we check:
####model 1 goodness-of-Fit
logLik(model_1)
AIC(model_1)
BIC(model_1)
#model 1: Prediction on test data
AnnualEst_pred_model_1<-predict(model_1,test)
#model 1: training RMSE calculation
model_1_rmse_train<-(sum(model_1$residuals^2)/nrow(train))^0.5
#model 1: test RMSE calculation
model_1_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_1)^2)/nrow(test))^0.5
####model 2 Goodness-of-Fit
logLik(model_2)
AIC(model_2)
BIC(model_2)
#model 2: Prediction on train and test data and converting back from log
AnnualEst_pred_model_2_train<-exp(predict(model_2,train))
AnnualEst_pred_model_2_test<-exp(predict(model_2,test))
#model 2: training RMSE of not log, but back-transformed estimates
model_2_rmse_train<-(sum((train$AnnualEst-AnnualEst_pred_model_2_train)^2)/nrow(train))^0.5
#model 2: test RMSE
model_2_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_2_test)^2)/nrow(test))^0.5
## Display results
data.frame(Metric = c("R²", "Adjusted R²", "AIC", "BIC", "RMSE (Train)", "RMSE (Test)"),
Model_1 = c(summary(model_1)$r.squared, summary(model_1)$adj.r.squared,
AIC(model_1), BIC(model_1), train_rmse1, test_rmse1),
Model_2 = c(summary(model_2)$r.squared, summary(model_2)$adj.r.squared,
AIC(model_2), BIC(model_2), train_rmse2, test_rmse2))
## Display results
result<-data.frame(Metric = c("R²", "Adjusted R²", "AIC", "BIC", "RMSE (Train)", "RMSE (Test)"),
Model_1 = c(summary(model_1)$r.squared, summary(model_1)$adj.r.squared,
AIC(model_1), BIC(model_1), model_1_rmse_train, model_1_rmse_test),
Model_2 = c(summary(model_2)$r.squared, summary(model_2)$adj.r.squared,
AIC(model_2), BIC(model_2), model_2_rmse_train, model_2_rmse_test))
result
###QUESTION 2####
district_summary <- train %>%
group_by(District) %>%
summarize(
Mean = mean(AnnualEst, na.rm = TRUE),
SD = sd(AnnualEst, na.rm = TRUE),
Lower_CI = Mean - 1.96 * (SD / sqrt(n())),
Upper_CI = Mean + 1.96 * (SD / sqrt(n()))
)
print(district_summary)
###QUESTION 4#####
d4 <- train %>% filter(District == 4) %>% pull(AnnualEst)
d7 <- train %>% filter(District == 7) %>% pull(AnnualEst)
t_test_result <- t.test(d4, d7, var.equal = TRUE)
print(t_test_result)
###QUESTION 6####
vars <- train %>% select(PopT, WalkComT, HseHldT, EmpT, StSegT)
corr_matrix <- cor(vars, use = "complete.obs")
corrplot(corr_matrix, method = "circle")
###QUESTION 6###
# Revised Model 1 (Linear)
revised_model1 <- lm(AnnualEst ~ PopT + WalkComT + EmpT, data = train)
summary(revised_model1)
# Revised Model 2 (Log-Linear)
revised_model2 <- lm(logAnnualEst ~ PopT + WalkComT + EmpT, data = train)
summary(revised_model2)
library(tidyverse)
#install.packages("corrplot")
library(corrplot)
#Import data
pedvars <- read.csv("ped_exposure_data_sample.csv")
colnames(pedvars)
# get training and test data (80-20 split)
n<-nrow(pedvars)
set.seed(12345) ##changing the seed will change the training/test data sets
index <- sample(n, round(n*0.8))
train <- pedvars[index,] #80% of data
test <- pedvars[-index,] #20% of data
##########Initial models#########
#Modeling AnnualEst#########
model_1 <- lm(AnnualEst~PopT,
data=train)
summary(model_1)
#Modeling AnnualEst#########
model_2 <- lm(AnnualEst~HseHldT,
data=train)
summary(model_2)
model_3 <- lm(AnnualEst~PopT+HseHldT,
data=train)
summary(model_3)
###Modeling logAnnualEst#########
model_2 <- lm(logAnnualEst~PopT,
data=train)
summary(model_2)
####model 1 goodness-of-Fit
logLik(model_1)
AIC(model_1)
BIC(model_1)
#model 1: Prediction on test data
AnnualEst_pred_model_1<-predict(model_1,test)
#model 1: training RMSE
model_1_rmse_train<-(sum(model_1$residuals^2)/nrow(train))^0.5
#model 1: test RMSE
model_1_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_1)^2)/nrow(test))^0.5
####model 2 Goodness-of-Fit
logLik(model_2)
AIC(model_2)
BIC(model_2)
#model 2: Prediction on train and test data and converting back from log
AnnualEst_pred_model_2_train<-exp(predict(model_2,train))
AnnualEst_pred_model_2_test<-exp(predict(model_2,test))
#model 2: training RMSE of not log, but back-transformed estimates
model_2_rmse_train<-(sum((train$AnnualEst-AnnualEst_pred_model_2_train)^2)/nrow(train))^0.5
#model 2: test RMSE
model_2_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_2_test)^2)/nrow(test))^0.5
###corrplot
colnames(train)
vars<-train[,6:21]
corrmat<-cor(vars)
corrplot(corrmat) # default is circle
##group_by summarize
volume_by_signal<-train%>%group_by(Signal) %>%
summarize(n=n(),mean_AADT=mean(AnnualEst),sd_AADT=sd(AnnualEst))%>%
as.data.frame()
volume_by_signal
setwd("C:/Users/Hp/Downloads/CE687_Assignment_1_ped_data_variables_code_c8ceed0a-9566-4969-b8cc-50a05fcfa32c/Assignment_1_ped_data_variables_code")#(i) Model Selection & Justification
getwd()
library(tidyverse)
#install.packages("corrplot")
library(corrplot)
#Import data
pedvars <- read.csv("ped_exposure_data_sample.csv")
colnames(pedvars)
setwd("C:/Users/Hp/Downloads/CE687_Assignment_1_ped_data_variables_code_c8ceed0a-9566-4969-b8cc-50a05fcfa32c/Assignment_1_ped_data_variables_code")#(i) Model Selection & Justification
getwd()
library(tidyverse)
install.packages("corrplot")
install.packages("corrplot")
library(corrplot)
#Import data
pedvars <- read.csv("ped_exposure_data_sample.csv")
colnames(pedvars)
# split data into training and test data (80-20 split)
n<-nrow(pedvars)
set.seed(12345) ##changing the seed will change the training/test data sets
index <- sample(n, round(n*0.8))
train <- pedvars[index,] #80% of data
test <- pedvars[-index,] #20% of data
##Comparing Model 1 (Linear) and Model 2 (Log-Linear).
#Model 1: Linear Regression (AnnualEst ~ PopT)
model_1 <- lm(AnnualEst~PopT,data=train)
summary(model_1)
###Model 2: Log-Linear Regression (logAnnualEst ~ PopT)
model_2 <- lm(logAnnualEst~PopT,data=train)
summary(model_2)
##To compare the models, we check:
####model 1 goodness-of-Fit
logLik(model_1)
AIC(model_1)
BIC(model_1)
#model 1: Prediction on test data
AnnualEst_pred_model_1<-predict(model_1,test)
#model 1: training RMSE calculation
model_1_rmse_train<-(sum(model_1$residuals^2)/nrow(train))^0.5
#model 1: test RMSE calculation
model_1_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_1)^2)/nrow(test))^0.5
####model 2 Goodness-of-Fit
logLik(model_2)
AIC(model_2)
BIC(model_2)
#model 2: Prediction on train and test data and converting back from log
AnnualEst_pred_model_2_train<-exp(predict(model_2,train))
AnnualEst_pred_model_2_test<-exp(predict(model_2,test))
#model 2: training RMSE of not log, but back-transformed estimates
model_2_rmse_train<-(sum((train$AnnualEst-AnnualEst_pred_model_2_train)^2)/nrow(train))^0.5
#model 2: test RMSE
model_2_rmse_test<-(sum((test$AnnualEst-AnnualEst_pred_model_2_test)^2)/nrow(test))^0.5
## Display results
result<-data.frame(Metric = c("R²", "Adjusted R²", "AIC", "BIC", "RMSE (Train)", "RMSE (Test)"),
Model_1 = c(summary(model_1)$r.squared, summary(model_1)$adj.r.squared,
AIC(model_1), BIC(model_1), model_1_rmse_train, model_1_rmse_test),
Model_2 = c(summary(model_2)$r.squared, summary(model_2)$adj.r.squared,
AIC(model_2), BIC(model_2), model_2_rmse_train, model_2_rmse_test))
result
###QUESTION 2 & 3####
district_summary <- train %>%
group_by(District) %>%
summarize(
Mean = mean(AnnualEst, na.rm = TRUE),
SD = sd(AnnualEst, na.rm = TRUE),
Lower_CI = Mean - 1.96 * (SD / sqrt(n())),
Upper_CI = Mean + 1.96 * (SD / sqrt(n()))
)
print(district_summary)
###QUESTION 4#####
d4 <- train %>% filter(District == 4) %>% pull(AnnualEst)
d7 <- train %>% filter(District == 7) %>% pull(AnnualEst)
t_test_result <- t.test(d4, d7, var.equal = TRUE)
print(t_test_result)
###QUESTION 5####
vars <- train %>% select(PopT, WalkComT, HseHldT, EmpT, StSegT)
corr_matrix <- cor(vars, use = "complete.obs")
corrplot(corr_matrix, method = "circle")
###QUESTION 6###
# Revised Model 1 (Linear)
revised_model1 <- lm(AnnualEst ~ PopT + WalkComT + EmpT, data = train)
summary(revised_model1)
# Revised Model 2 (Log-Linear)
revised_model2 <- lm(logAnnualEst ~ PopT + WalkComT + EmpT, data = train)
summary(revised_model2)
colnames(train)
vars<-train[,6:21]
corrmat<-cor(vars)
corrplot(corrmat) # default is circle
###QUESTION 6###
# Revised Model 1 (Linear)
revised_model1 <- lm(AnnualEst ~ PopT + HseHldT + EmpT + StSegT, data = train)
summary(revised_model1)
# Revised Model 2 (Log-Linear)
revised_model2 <- lm(logAnnualEst ~ PopT + HseHldT + EmpT + StSegT, data = train)
summary(revised_model2)
